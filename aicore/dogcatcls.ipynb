{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1CpXxh5T02oFdDUCsoBZH6V6aCfqkZGtF","authorship_tag":"ABX9TyMQrYYvwOqteqqW8NHykxeF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Mcua18EPh4Gf","executionInfo":{"status":"ok","timestamp":1770089887391,"user_tz":-420,"elapsed":25073,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"2616b60c-fe6c-46e4-858e-6d8f740d172a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","metadata":{"id":"b1bb0553","executionInfo":{"status":"ok","timestamp":1770090047165,"user_tz":-420,"elapsed":26980,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}}},"source":["!unzip -q '/content/drive/MyDrive/datasetoxfordclass.zip' -d '/content/'"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9fca3286"},"source":["# Task\n","Fine-tune a MobileNetV2 model for image classification using the dataset located at `/content/` to classify images and summarize the fine-tuning process and its results."]},{"cell_type":"markdown","metadata":{"id":"af3caa0e"},"source":["## Khám phá tập dữ liệu\n","\n","### Subtask:\n","Liệt kê nội dung của thư mục đã giải nén (`/content/`) để hiểu cấu trúc tập dữ liệu và xác định vị trí hình ảnh cũng như nhãn.\n"]},{"cell_type":"markdown","metadata":{"id":"94d78c48"},"source":["**Reasoning**:\n","I need to list the contents of the `/content/` directory to understand the dataset structure and identify where the images and labels are located, as per the first instruction.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b77451a0","executionInfo":{"status":"ok","timestamp":1770090149864,"user_tz":-420,"elapsed":126,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"33822c40-c181-4cd0-a5fa-d3c7338f2101"},"source":["import os\n","\n","# List the contents of the /content/ directory\n","print('Listing contents of /content/:')\n","!ls -F /content/"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Listing contents of /content/:\n","annotations/  drive/  images/  sample_data/\n"]}]},{"cell_type":"markdown","metadata":{"id":"02bb9e0e"},"source":["**Reasoning**:\n","The previous output showed `images/` and `annotations/` directories. To further understand the dataset structure and locate images, I will list the contents of the `images/` directory.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e703811b","executionInfo":{"status":"ok","timestamp":1770090155556,"user_tz":-420,"elapsed":107,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"5493823d-c927-42c1-f10d-0be39c6b5978"},"source":["print('Listing contents of /content/images/:')\n","!ls -F /content/images/"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Listing contents of /content/images/:\n","images/\n"]}]},{"cell_type":"markdown","metadata":{"id":"d028a2e9"},"source":["**Reasoning**:\n","The previous output indicated a nested directory structure (`/content/images/images/`). To find the actual image files and understand their arrangement, I need to list the contents of the innermost `images/` directory.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"695146ae","executionInfo":{"status":"ok","timestamp":1770090162008,"user_tz":-420,"elapsed":107,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"0bd668b1-22a2-4b7c-d0ea-cfec6eb44a1d"},"source":["print('Listing contents of /content/images/images/:')\n","!ls -F /content/images/images/ | head -n 10"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Listing contents of /content/images/images/:\n","Abyssinian_100.jpg\n","Abyssinian_100.mat\n","Abyssinian_101.jpg\n","Abyssinian_101.mat\n","Abyssinian_102.jpg\n","Abyssinian_102.mat\n","Abyssinian_103.jpg\n","Abyssinian_104.jpg\n","Abyssinian_105.jpg\n","Abyssinian_106.jpg\n"]}]},{"cell_type":"markdown","metadata":{"id":"366968be"},"source":["**Reasoning**:\n","The previous steps explored the image directory. Now, to understand the dataset structure comprehensively and locate the labels as per the subtask, I need to list the contents of the `annotations/` directory, which is likely to contain label information.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1dcc630e","executionInfo":{"status":"ok","timestamp":1770090171100,"user_tz":-420,"elapsed":108,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"0cde195b-ef7b-41b0-f4f5-6027f3bf9956"},"source":["print('Listing contents of /content/annotations/:')\n","!ls -F /content/annotations/ | head -n 10"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Listing contents of /content/annotations/:\n","annotations/\n"]}]},{"cell_type":"markdown","metadata":{"id":"0beb5032"},"source":["**Reasoning**:\n","The previous output for `/content/annotations/` showed a nested `annotations/` directory. To find the actual annotation files and understand their structure, I need to list the contents of the innermost `/content/annotations/annotations/` directory.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c65e938f","executionInfo":{"status":"ok","timestamp":1770090177307,"user_tz":-420,"elapsed":110,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"c11ff4a0-2f1c-46c6-e1fb-f7652d8c0215"},"source":["print('Listing contents of /content/annotations/annotations/:')\n","!ls -F /content/annotations/annotations/ | head -n 10"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Listing contents of /content/annotations/annotations/:\n","list.txt\n","README\n","test.txt\n","trainval.txt\n","trimaps/\n","xmls/\n"]}]},{"cell_type":"markdown","metadata":{"id":"0767c1c8"},"source":["## Chuẩn bị dữ liệu\n","\n","### Subtask:\n","Tạo các bộ tải dữ liệu (data loaders) cho tập huấn luyện và tập xác thực, bao gồm tải hình ảnh, thay đổi kích thước, chuẩn hóa và mã hóa nhãn.\n"]},{"cell_type":"markdown","metadata":{"id":"6d1a3139"},"source":["**Reasoning**:\n","I need to read the `list.txt` file located in `/content/annotations/annotations/` to extract all unique class names, which is the first part of the instruction to analyze annotation files and understand the dataset's class structure. This file typically contains class definitions or a list of categories.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b14b0aa3","executionInfo":{"status":"ok","timestamp":1770090203694,"user_tz":-420,"elapsed":65,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"86a8e7f3-e435-4e65-dd2d-7b83887f11f4"},"source":["import os\n","import re\n","\n","annotations_dir = '/content/annotations/annotations/'\n","list_file_path = os.path.join(annotations_dir, 'list.txt')\n","\n","# Read list.txt to extract class names and store them\n","class_names = []\n","with open(list_file_path, 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line.startswith('#'): # Skip comments\n","            continue\n","        # Example line: Abyssinian 1 1 1\n","        # Extract the breed name, which is the class name\n","        parts = line.split(' ')\n","        if parts:\n","            class_name = parts[0]\n","            class_names.append(class_name)\n","\n","# Get unique class names and sort them for consistent indexing\n","unique_class_names = sorted(list(set(class_names)))\n","num_classes = len(unique_class_names)\n","\n","# Create a mapping from class name to integer label\n","class_to_label = {name: i for i, name in enumerate(unique_class_names)}\n","label_to_class = {i: name for i, name in enumerate(unique_class_names)}\n","\n","print(f\"Number of unique classes: {num_classes}\")\n","print(\"First 5 class names:\")\n","for i in range(min(5, num_classes)):\n","    print(f\"  {unique_class_names[i]}\")\n","print(\"Class to label mapping (first 5):\")\n","for i, (k, v) in enumerate(class_to_label.items()):\n","    if i >= 5: break\n","    print(f\"  '{k}': {v}\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of unique classes: 7349\n","First 5 class names:\n","  Abyssinian_1\n","  Abyssinian_10\n","  Abyssinian_100\n","  Abyssinian_101\n","  Abyssinian_102\n","Class to label mapping (first 5):\n","  'Abyssinian_1': 0\n","  'Abyssinian_10': 1\n","  'Abyssinian_100': 2\n","  'Abyssinian_101': 3\n","  'Abyssinian_102': 4\n"]}]},{"cell_type":"markdown","metadata":{"id":"b650ee24"},"source":["**Reasoning**:\n","Following the first instruction of the subtask, I have already processed `list.txt`. Now, I need to read and parse `trainval.txt` and `test.txt` to extract image filenames and their corresponding class labels. This will create the necessary data for the training, validation, and test sets.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":903},"id":"963e9dee","executionInfo":{"status":"error","timestamp":1770090224571,"user_tz":-420,"elapsed":2961,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"72ed4305-c72f-48fa-e909-a5acd222c4c6"},"source":["import os\n","\n","image_base_path = '/content/images/images/'\n","\n","def parse_annotation_file(file_path, class_to_label):\n","    image_paths = []\n","    labels = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('#'):\n","                continue\n","            # Example line: Abyssinian_100 1 1 1\n","            parts = line.split(' ')\n","            if len(parts) >= 1:\n","                image_name_prefix = parts[0]\n","                # The image file names are typically <class_name>_<image_number>.jpg\n","                # The class name itself is the prefix before the last underscore, e.g., 'Abyssinian'\n","                # Need to find the actual class name based on the structure.\n","                # From previous step, list.txt entries were like 'Abyssinian_100', which might be a breed instance.\n","                # However, the task implies classification of broader categories (breeds).\n","                # Let's re-evaluate how class names are extracted from the image_name_prefix.\n","\n","                # Looking at typical Oxford-IIIT Pet dataset format:\n","                # Image file names: <breed_name>_<image_id>.jpg (e.g., Abyssinian_1.jpg)\n","                # Annotation lines: <image_name> <class_id> <species> <breed_id>\n","                # The problem statement states 'fine-tune a MobileNetV2 model for image classification'.\n","                # The `list.txt` processing resulted in 7349 unique classes like 'Abyssinian_1', 'Abyssinian_10'.\n","                # This suggests that each image instance might be treated as a unique class, which is unusual for breed classification.\n","                # Let's assume for now the *actual* class name is the breed name before the last underscore.\n","\n","                # Revised approach to extract class name from image_name_prefix:\n","                # Example: 'Abyssinian_100' -> actual breed name 'Abyssinian'\n","                # We need to map this 'Abyssinian' to a label.\n","\n","                # Let's re-examine the structure of list.txt and common practice for Oxford-IIIT Pet.\n","                # list.txt has lines like: 'Abyssinian_1 1 1 1'\n","                # The first part 'Abyssinian_1' is the image filename prefix.\n","                # The actual breed name is usually the part before the last underscore, e.g., 'Abyssinian'.\n","                # Let's define the class names based on the *breed* rather than the instance.\n","\n","                # Let's rebuild unique_class_names and class_to_label to reflect actual breeds if that was the intent.\n","                # However, the previous `class_to_label` created 7349 classes from `list.txt` based on `parts[0]`.\n","                # If the goal is to classify 37 breeds, then `num_classes` should be 37.\n","                # The original `list.txt` processing seems to have interpreted each distinct image prefix as a class.\n","                # This needs correction if the goal is breed classification.\n","\n","                # Let's stick to the output of `list.txt` processing for now, which created labels for specific image prefixes.\n","                # The problem statement says 'classify images' and 'fine-tune a MobileNetV2 model for image classification'.\n","                # It does not explicitly state 'breed classification'.\n","                # The number of classes (7349) is very high for standard image classification datasets.\n","                # It's more likely that the output `Abyssinian_1`, `Abyssinian_10` are the *class labels* themselves (e.g., specific animals).\n","                # This interpretation would mean the dataset is about individual identification rather than breed.\n","                # Given the `list.txt` structure, where each entry is unique and contains an 'image name prefix' as the first part,\n","                # it implies `parts[0]` *is* the class name for that image.\n","\n","                # Let's assume `parts[0]` from `trainval.txt` and `test.txt` directly correspond to the keys in `class_to_label`.\n","\n","                image_name_prefix = parts[0]\n","                class_idx = class_to_label[image_name_prefix]\n","\n","                # Construct the full image path\n","                full_image_path = os.path.join(image_base_path, image_name_prefix + '.jpg')\n","\n","                image_paths.append(full_image_path)\n","                labels.append(class_idx)\n","    return image_paths, labels\n","\n","# Paths to trainval.txt and test.txt\n","trainval_file_path = os.path.join(annotations_dir, 'trainval.txt')\n","test_file_path = os.path.join(annotations_dir, 'test.txt')\n","\n","# Parse trainval.txt\n","trainval_image_paths, trainval_labels = parse_annotation_file(trainval_file_path, class_to_label)\n","print(f\"Found {len(trainval_image_paths)} images for training/validation.\")\n","print(f\"First 5 training/validation image paths and labels:\")\n","for i in range(min(5, len(trainval_image_paths))):\n","    print(f\"  Path: {trainval_image_paths[i]}, Label: {trainval_labels[i]} ({label_to_class[trainval_labels[i]]})\")\n","\n","# Parse test.txt\n","test_image_paths, test_labels = parse_annotation_file(test_file_path, class_to_label)\n","print(f\"Found {len(test_image_paths)} images for testing.\")\n","print(f\"First 5 testing image paths and labels:\")\n","for i in range(min(5, len(test_image_paths))):\n","    print(f\"  Path: {test_image_paths[i]}, Label: {test_labels[i]} ({label_to_class[test_labels[i]]})\")\n","\n","\n","# Now, split the trainval set into actual training and validation sets\n","from sklearn.model_selection import train_test_split\n","\n","train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n","    trainval_image_paths, trainval_labels, test_size=0.2, random_state=42, stratify=trainval_labels\n",")\n","\n","print(f\"\\nAfter splitting trainval set:\")\n","print(f\"  Training images: {len(train_image_paths)}\")\n","print(f\"  Validation images: {len(val_image_paths)}\")\n","print(f\"  Test images: {len(test_image_paths)}\")"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 3680 images for training/validation.\n","First 5 training/validation image paths and labels:\n","  Path: /content/images/images/Abyssinian_100.jpg, Label: 2 (Abyssinian_100)\n","  Path: /content/images/images/Abyssinian_101.jpg, Label: 3 (Abyssinian_101)\n","  Path: /content/images/images/Abyssinian_102.jpg, Label: 4 (Abyssinian_102)\n","  Path: /content/images/images/Abyssinian_103.jpg, Label: 5 (Abyssinian_103)\n","  Path: /content/images/images/Abyssinian_104.jpg, Label: 6 (Abyssinian_104)\n","Found 3669 images for testing.\n","First 5 testing image paths and labels:\n","  Path: /content/images/images/Abyssinian_201.jpg, Label: 102 (Abyssinian_201)\n","  Path: /content/images/images/Abyssinian_202.jpg, Label: 103 (Abyssinian_202)\n","  Path: /content/images/images/Abyssinian_204.jpg, Label: 104 (Abyssinian_204)\n","  Path: /content/images/images/Abyssinian_205.jpg, Label: 105 (Abyssinian_205)\n","  Path: /content/images/images/Abyssinian_206.jpg, Label: 106 (Abyssinian_206)\n"]},{"output_type":"error","ename":"ValueError","evalue":"The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-146154202.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0mtrainval_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainval_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainval_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2870\u001b[0m         \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2872\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstratify\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2874\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_common_namespace_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   1907\u001b[0m         \"\"\"\n\u001b[1;32m   1908\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1909\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1910\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_iter_indices\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[0mclass_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_counts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2318\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2319\u001b[0m                 \u001b[0;34m\"The least populated class in y has only 1\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2320\u001b[0m                 \u001b[0;34m\" member, which is too few. The minimum\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."]}]},{"cell_type":"markdown","metadata":{"id":"d9758f49"},"source":["**Reasoning**:\n","The previous `train_test_split` failed because many 'classes' had only one sample, which is incompatible with stratified splitting. This was due to incorrectly identifying each unique image identifier (e.g., 'Abyssinian_100') as a separate class. The Oxford-IIIT Pet dataset typically classifies by breed. I need to modify the class extraction logic to identify actual breed names (e.g., 'Abyssinian') as classes, which will result in a reasonable number of classes (around 37) and allow for successful stratified splitting.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f0e9cee2","executionInfo":{"status":"ok","timestamp":1770090242704,"user_tz":-420,"elapsed":292,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"bfa33afd-9191-422a-a0a7-7ded5bddbc25"},"source":["import os\n","import re\n","from sklearn.model_selection import train_test_split\n","\n","annotations_dir = '/content/annotations/annotations/'\n","list_file_path = os.path.join(annotations_dir, 'list.txt')\n","\n","# Step 1: Re-evaluate class extraction to get actual breed names\n","# The original list.txt interpretation created too many classes (individual image identifiers)\n","# For Oxford-IIIT Pet, classes are typically breeds.\n","# Let's derive breed names from the entries in list.txt (or image filenames).\n","\n","# Function to extract breed name from a string like 'Abyssinian_100'\n","def extract_breed_name(image_name_prefix):\n","    # The breed name is everything before the last underscore\n","    return '_'.join(image_name_prefix.split('_')[:-1])\n","\n","# Read list.txt to extract actual breed names\n","breed_names = []\n","with open(list_file_path, 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line.startswith('#'): # Skip comments\n","            continue\n","        parts = line.split(' ')\n","        if parts:\n","            image_name_prefix = parts[0]\n","            breed_name = extract_breed_name(image_name_prefix)\n","            if breed_name:\n","                breed_names.append(breed_name)\n","\n","# Get unique breed names and sort them for consistent indexing\n","unique_breed_names = sorted(list(set(breed_names)))\n","num_classes = len(unique_breed_names)\n","\n","# Create a mapping from breed name to integer label\n","class_to_label = {name: i for i, name in enumerate(unique_breed_names)}\n","label_to_class = {i: name for i, name in enumerate(unique_breed_names)}\n","\n","print(f\"Number of actual breed classes: {num_classes}\")\n","print(\"First 5 breed names:\")\n","for i in range(min(5, num_classes)):\n","    print(f\"  {unique_breed_names[i]}\")\n","print(\"Class to label mapping (first 5):\")\n","for i, (k, v) in enumerate(class_to_label.items()):\n","    if i >= 5: break\n","    print(f\"  '{k}': {v}\")\n","\n","image_base_path = '/content/images/images/'\n","\n","def parse_annotation_file(file_path, class_to_label):\n","    image_paths = []\n","    labels = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('#'):\n","                continue\n","            parts = line.split(' ')\n","            if len(parts) >= 1:\n","                image_name_prefix = parts[0]\n","                breed_name = extract_breed_name(image_name_prefix)\n","\n","                if breed_name in class_to_label:\n","                    class_idx = class_to_label[breed_name]\n","                    full_image_path = os.path.join(image_base_path, image_name_prefix + '.jpg')\n","\n","                    # Check if the image file actually exists\n","                    if os.path.exists(full_image_path):\n","                        image_paths.append(full_image_path)\n","                        labels.append(class_idx)\n","                    # else: # Optional: print a warning if image not found\n","                    #     print(f\"Warning: Image file not found: {full_image_path}\")\n","                # else: # Optional: print a warning if breed name not in our defined classes\n","                #     print(f\"Warning: Breed name '{breed_name}' not found in class_to_label for image '{image_name_prefix}'\")\n","    return image_paths, labels\n","\n","# Paths to trainval.txt and test.txt\n","trainval_file_path = os.path.join(annotations_dir, 'trainval.txt')\n","test_file_path = os.path.join(annotations_dir, 'test.txt')\n","\n","# Parse trainval.txt\n","trainval_image_paths, trainval_labels = parse_annotation_file(trainval_file_path, class_to_label)\n","print(f\"\\nFound {len(trainval_image_paths)} images for training/validation (based on actual breeds).\")\n","print(f\"First 5 training/validation image paths and labels:\")\n","for i in range(min(5, len(trainval_image_paths))):\n","    print(f\"  Path: {trainval_image_paths[i]}, Label: {trainval_labels[i]} ({label_to_class[trainval_labels[i]]})\")\n","\n","# Parse test.txt\n","test_image_paths, test_labels = parse_annotation_file(test_file_path, class_to_label)\n","print(f\"\\nFound {len(test_image_paths)} images for testing (based on actual breeds).\")\n","print(f\"First 5 testing image paths and labels:\")\n","for i in range(min(5, len(test_image_paths))):\n","    print(f\"  Path: {test_image_paths[i]}, Label: {test_labels[i]} ({label_to_class[test_labels[i]]})\")\n","\n","\n","# Now, split the trainval set into actual training and validation sets\n","# With fewer, correctly defined classes, stratification should now work.\n","train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n","    trainval_image_paths, trainval_labels, test_size=0.2, random_state=42, stratify=trainval_labels\n",")\n","\n","print(f\"\\nAfter splitting trainval set:\")\n","print(f\"  Training images: {len(train_image_paths)}\")\n","print(f\"  Validation images: {len(val_image_paths)}\")\n","print(f\"  Test images: {len(test_image_paths)}\")\n"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of actual breed classes: 37\n","First 5 breed names:\n","  Abyssinian\n","  Bengal\n","  Birman\n","  Bombay\n","  British_Shorthair\n","Class to label mapping (first 5):\n","  'Abyssinian': 0\n","  'Bengal': 1\n","  'Birman': 2\n","  'Bombay': 3\n","  'British_Shorthair': 4\n","\n","Found 3680 images for training/validation (based on actual breeds).\n","First 5 training/validation image paths and labels:\n","  Path: /content/images/images/Abyssinian_100.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_101.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_102.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_103.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_104.jpg, Label: 0 (Abyssinian)\n","\n","Found 3669 images for testing (based on actual breeds).\n","First 5 testing image paths and labels:\n","  Path: /content/images/images/Abyssinian_201.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_202.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_204.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_205.jpg, Label: 0 (Abyssinian)\n","  Path: /content/images/images/Abyssinian_206.jpg, Label: 0 (Abyssinian)\n","\n","After splitting trainval set:\n","  Training images: 2944\n","  Validation images: 736\n","  Test images: 3669\n"]}]},{"cell_type":"markdown","metadata":{"id":"9cd46842"},"source":["**Reasoning**:\n","The previous steps successfully extracted image paths and labels for training, validation, and testing. The next step is to define an image preprocessing function and create TensorFlow dataset objects as per instructions 4, 5, and 6 of the subtask. This involves reading the image, decoding it, resizing it to the MobileNetV2 input size (224x224), and normalizing pixel values. Then, `tf.data.Dataset` objects will be created, mapping the preprocessing function and configuring them for optimal performance (shuffling, batching, prefetching).\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3907940b","executionInfo":{"status":"ok","timestamp":1770090285829,"user_tz":-420,"elapsed":30987,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"a526f00f-ba99-44dd-e555-c6a69988e0d8"},"source":["import tensorflow as tf\n","\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","BATCH_SIZE = 32\n","\n","# Define the preprocessing function\n","def preprocess_image(image_path, label):\n","    # Read the image file\n","    image = tf.io.read_file(image_path)\n","    # Decode the image to a tensor\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    # Resize the image to the desired size for MobileNetV2\n","    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n","    # Normalize pixel values to the range [-1, 1]\n","    # MobileNetV2 expects input pixel values to be in the range [-1, 1]\n","    image = (image / 127.5) - 1\n","    return image, label\n","\n","# Create tf.data.Dataset objects for training, validation, and test sets\n","def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n","    # Create a dataset from image paths and labels\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","\n","    # Apply preprocessing\n","    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","\n","    # Configure for performance\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(image_paths))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","# Create datasets\n","train_dataset = create_dataset(train_image_paths, train_labels, shuffle=True)\n","val_dataset = create_dataset(val_image_paths, val_labels, shuffle=False)\n","test_dataset = create_dataset(test_image_paths, test_labels, shuffle=False)\n","\n","print(f\"Train dataset created with batch size {BATCH_SIZE}\")\n","print(f\"Validation dataset created with batch size {BATCH_SIZE}\")\n","print(f\"Test dataset created with batch size {BATCH_SIZE}\")\n","\n","# Verify a batch from the training dataset\n","for images, labels in train_dataset.take(1):\n","    print(f\"Sample batch shape - Images: {images.shape}, Labels: {labels.shape}\")\n","    break\n"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Train dataset created with batch size 32\n","Validation dataset created with batch size 32\n","Test dataset created with batch size 32\n","Sample batch shape - Images: (32, 224, 224, 3), Labels: (32,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"413e1301"},"source":["## Tải mô hình MobileNetV2\n","\n","### Subtask:\n","Tải mô hình MobileNetV2 đã được huấn luyện trước (pre-trained) từ TensorFlow/Keras, loại bỏ lớp phân loại trên cùng.\n"]},{"cell_type":"markdown","metadata":{"id":"6a7ba097"},"source":["**Reasoning**:\n","As per the subtask instruction, I need to load the pre-trained MobileNetV2 model from TensorFlow/Keras, ensuring that the top classification layer is excluded and ImageNet weights are used. I will use the previously defined IMG_HEIGHT and IMG_WIDTH for the input shape.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8dfc983f","executionInfo":{"status":"ok","timestamp":1770090317595,"user_tz":-420,"elapsed":4877,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"6cbd7e72-8231-41f3-f3f6-856b1bd4a1c8"},"source":["import tensorflow as tf\n","\n","# Import MobileNetV2\n","from tensorflow.keras.applications import MobileNetV2\n","\n","# Instantiate the MobileNetV2 model with pre-trained ImageNet weights,\n","# excluding the top classification layer.\n","# The input_shape is derived from the previously defined IMG_HEIGHT and IMG_WIDTH.\n","base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n","                         include_top=False,\n","                         weights='imagenet')\n","\n","print(\"MobileNetV2 base model loaded successfully with ImageNet weights and without the top layer.\")\n","print(f\"Base model output shape: {base_model.output_shape}\")"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","MobileNetV2 base model loaded successfully with ImageNet weights and without the top layer.\n","Base model output shape: (None, 7, 7, 1280)\n"]}]},{"cell_type":"markdown","metadata":{"id":"1586a503"},"source":["## Sửa đổi mô hình để tinh chỉnh\n","\n","### Subtask:\n","Thêm các lớp phân loại mới phù hợp với số lượng lớp duy nhất trong tập dữ liệu của bạn.\n"]},{"cell_type":"markdown","metadata":{"id":"0387a8df"},"source":["**Reasoning**:\n","As per the instructions, I need to add new classification layers on top of the pre-trained MobileNetV2 base model. This involves freezing the base model, defining a new sequential head with GlobalAveragePooling2D, a dense layer, and a final dense classification layer with softmax activation, and then combining them into a final model. Finally, I will print the model summary.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"cc516e8b","executionInfo":{"status":"ok","timestamp":1770090336533,"user_tz":-420,"elapsed":132,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"1843688a-9d45-4702-c606-516b1e67d7d9"},"source":["import tensorflow as tf\n","\n","# 1. Freeze the base model\n","base_model.trainable = False\n","\n","# 2. Create a new classification head\n","# The number of classes was determined in the previous step (num_classes)\n","\n","head = tf.keras.Sequential([\n","    tf.keras.layers.GlobalAveragePooling2D(), # Converts the 7x7x1280 feature map into a 1280-element vector\n","    tf.keras.layers.Dense(128, activation='relu'), # A dense hidden layer\n","    tf.keras.layers.Dense(num_classes, activation='softmax') # The final classification layer\n","])\n","\n","# 6. Create the complete model by connecting the base_model and the head\n","model = tf.keras.Sequential([\n","    base_model,\n","    head\n","])\n","\n","# 7. Print the model summary to verify the new architecture\n","print(\"Model architecture after adding classification head:\")\n","model.summary()"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Model architecture after adding classification head:\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential_1\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n","│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ sequential (\u001b[38;5;33mSequential\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m37\u001b[0m)             │       \u001b[38;5;34m168,741\u001b[0m │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n","│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n","│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n","├─────────────────────────────────┼────────────────────────┼───────────────┤\n","│ sequential (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">37</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">168,741</span> │\n","└─────────────────────────────────┴────────────────────────┴───────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,426,725\u001b[0m (9.26 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,426,725</span> (9.26 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m168,741\u001b[0m (659.14 KB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">168,741</span> (659.14 KB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"794c2574"},"source":["## Biên dịch mô hình\n","\n","### Subtask:\n","Cấu hình mô hình với trình tối ưu hóa (optimizer), hàm mất mát (loss function) và các chỉ số (metrics) phù hợp để huấn luyện.\n"]},{"cell_type":"markdown","metadata":{"id":"4a00cd83"},"source":["**Reasoning**:\n","As per the instructions, I need to configure the model by selecting an optimizer (Adam), a loss function (`SparseCategoricalCrossentropy` for multi-class classification with integer labels and softmax output), and metrics (accuracy) and then compile the model using these.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"314d5244","executionInfo":{"status":"ok","timestamp":1770090356433,"user_tz":-420,"elapsed":40,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"78ab3281-7274-4568-a2cd-616802782af0"},"source":["import tensorflow as tf\n","\n","# 1. Choose an optimizer\n","optimizer = tf.keras.optimizers.Adam()\n","\n","# 2. Choose a loss function suitable for multi-class classification\n","# Since our labels are integer-encoded (0, 1, 2, ...) and the output layer uses 'softmax' activation,\n","# SparseCategoricalCrossentropy is the correct choice, with from_logits=False.\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","\n","# 3. Choose metrics to monitor model performance\n","metrics = ['accuracy']\n","\n","# 4. Compile the model\n","model.compile(optimizer=optimizer,\n","              loss=loss_fn,\n","              metrics=metrics)\n","\n","print(\"Model compiled successfully with Adam optimizer, SparseCategoricalCrossentropy loss, and accuracy metric.\")"],"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model compiled successfully with Adam optimizer, SparseCategoricalCrossentropy loss, and accuracy metric.\n"]}]},{"cell_type":"markdown","metadata":{"id":"77d31837"},"source":["## Tinh chỉnh mô hình\n","\n","### Subtask:\n","Tiến hành huấn luyện mô hình MobileNetV2 đã sửa đổi trên tập dữ liệu đã chuẩn bị.\n"]},{"cell_type":"markdown","metadata":{"id":"4f1dfd6a"},"source":["**Reasoning**:\n","As per the instructions, I need to train the compiled model using the prepared training and validation datasets. I will specify a suitable number of epochs and store the training history.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"578642e5","executionInfo":{"status":"error","timestamp":1770090914958,"user_tz":-420,"elapsed":28,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"6faeb807-1a3b-4ae4-dcd3-0fb45c531379"},"source":["EPOCHS = 10 # Define the number of training epochs\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"Model training completed for {EPOCHS} epochs.\")"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1801377605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"c153c932"},"source":["# Task\n","```python\n","import tensorflow as tf\n","\n","# Freeze the base model if it's not already frozen (ensures transfer learning)\n","base_model.trainable = False\n","\n","# Create a new classification head (re-using num_classes from previous steps)\n","head = tf.keras.Sequential([\n","    tf.keras.layers.GlobalAveragePooling2D(),\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","# Create the complete model by connecting the base_model and the head\n","model = tf.keras.Sequential([\n","    base_model,\n","    head\n","])\n","\n","print(\"Model architecture after adding classification head:\")\n","model.summary()\n","\n","# Compile the model\n","optimizer = tf.keras.optimizers.Adam()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","metrics = ['accuracy']\n","\n","model.compile(optimizer=optimizer,\n","              loss=loss_fn,\n","              metrics=metrics)\n","\n","print(\"\\nModel compiled successfully with Adam optimizer, SparseCategoricalCrossentropy loss, and accuracy metric.\")\n","\n","# Define the number of training epochs\n","EPOCHS = 10\n","\n","# Train the model\n","print(f\"\\nStarting model training for {EPOCHS} epochs...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"\\nModel training completed for {EPOCHS} epochs.\")\n","```"]},{"cell_type":"markdown","metadata":{"id":"5a7a0e68"},"source":["## Thiết lập GPU\n","\n","### Subtask:\n","Kiểm tra và cấu hình môi trường để sử dụng GPU nhằm tăng tốc quá trình huấn luyện.\n"]},{"cell_type":"markdown","metadata":{"id":"615317b5"},"source":["**Reasoning**:\n","The previous code block resulted in a `NameError` because the `model` object was not defined when the cell was executed. This indicates a state loss or an issue with the execution order. Before proceeding with training, I need to ensure the `model` object is defined.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":358},"id":"e08a6791","executionInfo":{"status":"error","timestamp":1770091062593,"user_tz":-420,"elapsed":11859,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"e27ed750-633b-4bc8-9249-c04d9d449389"},"source":["import tensorflow as tf\n","\n","# Import MobileNetV2\n","from tensorflow.keras.applications import MobileNetV2\n","\n","# Re-define IMG_HEIGHT, IMG_WIDTH, num_classes if they are not in the current kernel state\n","# Assuming these were defined in previous successful steps, but for robustness, include them if necessary.\n","# For example, if you ran the notebook from scratch up to the error, these should be defined.\n","# If not, you might need to re-run the cells that define them.\n","# Let's assume for now that num_classes, IMG_HEIGHT, IMG_WIDTH are available from previous successful runs.\n","\n","# Placeholder values if not available (ideally should come from previous executed cells)\n","# num_classes = 37 # Should be derived from actual class parsing\n","# IMG_HEIGHT = 224\n","# IMG_WIDTH = 224\n","\n","# Instantiate the MobileNetV2 model with pre-trained ImageNet weights,\n","# excluding the top classification layer.\n","base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n","                         include_top=False,\n","                         weights='imagenet')\n","\n","# Freeze the base model\n","base_model.trainable = False\n","\n","# Create a new classification head\n","head = tf.keras.Sequential([\n","    tf.keras.layers.GlobalAveragePooling2D(), # Converts the 7x7x1280 feature map into a 1280-element vector\n","    tf.keras.layers.Dense(128, activation='relu'), # A dense hidden layer\n","    tf.keras.layers.Dense(num_classes, activation='softmax') # The final classification layer\n","])\n","\n","# Create the complete model by connecting the base_model and the head\n","model = tf.keras.Sequential([\n","    base_model,\n","    head\n","])\n","\n","# Compile the model (assuming optimizer, loss_fn, metrics are available)\n","optimizer = tf.keras.optimizers.Adam()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","metrics = ['accuracy']\n","\n","model.compile(optimizer=optimizer,\n","              loss=loss_fn,\n","              metrics=metrics)\n","\n","print(\"Model rebuilt and compiled successfully.\")\n","print(f\"Number of classes used for the final layer: {num_classes}\")"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'IMG_HEIGHT' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-574868057.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Instantiate the MobileNetV2 model with pre-trained ImageNet weights,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# excluding the top classification layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n\u001b[0m\u001b[1;32m     20\u001b[0m                          \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                          weights='imagenet')\n","\u001b[0;31mNameError\u001b[0m: name 'IMG_HEIGHT' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"f4708ef2"},"source":["**Reasoning**:\n","The previous code block failed because `IMG_HEIGHT`, `IMG_WIDTH`, and `num_classes` were not defined in the current execution scope. I need to re-declare these variables with their established values before initializing and compiling the model.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1073fd83","executionInfo":{"status":"ok","timestamp":1770091081152,"user_tz":-420,"elapsed":3754,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"f5b48930-b9e2-4fd3-ccae-d367a61da428"},"source":["import tensorflow as tf\n","\n","# Import MobileNetV2\n","from tensorflow.keras.applications import MobileNetV2\n","\n","# Re-define IMG_HEIGHT, IMG_WIDTH, num_classes from previous successful steps\n","# These values were determined in cells 3907940b and f0e9cee2 respectively.\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","num_classes = 37 # This was determined after correctly parsing breed names\n","\n","# Instantiate the MobileNetV2 model with pre-trained ImageNet weights,\n","# excluding the top classification layer.\n","base_model = MobileNetV2(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n","                         include_top=False,\n","                         weights='imagenet')\n","\n","# Freeze the base model\n","base_model.trainable = False\n","\n","# Create a new classification head\n","head = tf.keras.Sequential([\n","    tf.keras.layers.GlobalAveragePooling2D(), # Converts the 7x7x1280 feature map into a 1280-element vector\n","    tf.keras.layers.Dense(128, activation='relu'), # A dense hidden layer\n","    tf.keras.layers.Dense(num_classes, activation='softmax') # The final classification layer\n","])\n","\n","# Create the complete model by connecting the base_model and the head\n","model = tf.keras.Sequential([\n","    base_model,\n","    head\n","])\n","\n","# Compile the model (assuming optimizer, loss_fn, metrics are available)\n","optimizer = tf.keras.optimizers.Adam()\n","loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n","metrics = ['accuracy']\n","\n","model.compile(optimizer=optimizer,\n","              loss=loss_fn,\n","              metrics=metrics)\n","\n","print(\"Model rebuilt and compiled successfully.\")\n","print(f\"Number of classes used for the final layer: {num_classes}\")"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n","\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Model rebuilt and compiled successfully.\n","Number of classes used for the final layer: 37\n"]}]},{"cell_type":"markdown","metadata":{"id":"4f82895b"},"source":["**Reasoning**:\n","As per the subtask instructions, I need to check for GPU availability. I will use `tf.config.list_physical_devices('GPU')` to list the physical GPU devices that TensorFlow can utilize.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2233e8c","executionInfo":{"status":"ok","timestamp":1770091090500,"user_tz":-420,"elapsed":114,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"a90410ed-b709-44a4-ca71-fed0176f76ca"},"source":["import tensorflow as tf\n","\n","gpus = tf.config.list_physical_devices('GPU')\n","if gpus:\n","    print(f\"Found {len(gpus)} GPU(s):\")\n","    for gpu in gpus:\n","        print(f\"  {gpu}\")\n","    print(\"GPU is available and configured for TensorFlow.\")\n","else:\n","    print(\"No GPU devices found. TensorFlow will run on CPU.\")\n","\n","# Optional: Run nvidia-smi for more detailed GPU information if a GPU is detected\n","if gpus:\n","    print(\"\\nRunning !nvidia-smi for detailed GPU information:\")\n","    !nvidia-smi"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 1 GPU(s):\n","  PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n","GPU is available and configured for TensorFlow.\n","\n","Running !nvidia-smi for detailed GPU information:\n","Tue Feb  3 03:58:10 2026       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0             27W /   70W |     134MiB /  15360MiB |      0%      Default |\n","|                                         |                        |                  N/A |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"712d7e8f"},"source":["## Gắn Google Drive và giải nén dữ liệu\n","\n","### Subtask:\n","Gắn Google Drive để truy cập tệp zip datasetoxfordclass.zip và giải nén nó vào thư mục /content/.\n"]},{"cell_type":"markdown","metadata":{"id":"db5efe5c"},"source":["## Tinh chỉnh mô hình\n","\n","### Subtask:\n","Tiến hành huấn luyện mô hình MobileNetV2 đã sửa đổi trên tập dữ liệu đã chuẩn bị.\n"]},{"cell_type":"markdown","metadata":{"id":"d64f29ec"},"source":["**Reasoning**:\n","The `model` object has been successfully compiled and `IMG_HEIGHT`, `IMG_WIDTH`, and `num_classes` are defined. The `train_dataset` and `val_dataset` are also ready. I will now proceed to train the model as per the instructions, defining `EPOCHS` and calling `model.fit()` to store the training history.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"75800e8c","executionInfo":{"status":"error","timestamp":1770091118917,"user_tz":-420,"elapsed":9,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"cf01c8ba-a742-4d33-dd78-7fad10faaa95"},"source":["EPOCHS = 10 # Define the number of training epochs\n","\n","# Train the model\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"Model training completed for {EPOCHS} epochs.\")"],"execution_count":5,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_dataset' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1801377605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m history = model.fit(\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"3257ce97"},"source":["**Reasoning**:\n","The previous code block failed because 'train_dataset' was not defined. This indicates that the data parsing and dataset creation steps were not executed or their variables were lost. I need to re-run the code for parsing annotation files, splitting data into training and validation sets, and creating TensorFlow dataset objects before initiating model training.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":305},"id":"80622e9a","executionInfo":{"status":"error","timestamp":1770091147764,"user_tz":-420,"elapsed":379,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"019a5165-e281-436b-e61b-5242060acf8c"},"source":["import os\n","import re\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","# --- Re-execute Data Parsing and Splitting (from f0e9cee2) ---\n","annotations_dir = '/content/annotations/annotations/'\n","list_file_path = os.path.join(annotations_dir, 'list.txt')\n","\n","def extract_breed_name(image_name_prefix):\n","    return '_'.join(image_name_prefix.split('_')[:-1])\n","\n","breed_names = []\n","with open(list_file_path, 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line.startswith('#'):\n","            continue\n","        parts = line.split(' ')\n","        if parts:\n","            image_name_prefix = parts[0]\n","            breed_name = extract_breed_name(image_name_prefix)\n","            if breed_name:\n","                breed_names.append(breed_name)\n","\n","unique_breed_names = sorted(list(set(breed_names)))\n","num_classes = len(unique_breed_names)\n","\n","class_to_label = {name: i for i, name in enumerate(unique_breed_names)}\n","label_to_class = {i: name for i, name in enumerate(unique_breed_names)}\n","\n","image_base_path = '/content/images/images/'\n","\n","def parse_annotation_file(file_path, class_to_label):\n","    image_paths = []\n","    labels = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('#'):\n","                continue\n","            parts = line.split(' ')\n","            if len(parts) >= 1:\n","                image_name_prefix = parts[0]\n","                breed_name = extract_breed_name(image_name_prefix)\n","\n","                if breed_name in class_to_label:\n","                    class_idx = class_to_label[breed_name]\n","                    full_image_path = os.path.join(image_base_path, image_name_prefix + '.jpg')\n","\n","                    if os.path.exists(full_image_path):\n","                        image_paths.append(full_image_path)\n","                        labels.append(class_idx)\n","    return image_paths, labels\n","\n","trainval_file_path = os.path.join(annotations_dir, 'trainval.txt')\n","test_file_path = os.path.join(annotations_dir, 'test.txt')\n","\n","trainval_image_paths, trainval_labels = parse_annotation_file(trainval_file_path, class_to_label)\n","test_image_paths, test_labels = parse_annotation_file(test_file_path, class_to_label)\n","\n","train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n","    trainval_image_paths, trainval_labels, test_size=0.2, random_state=42, stratify=trainval_labels\n",")\n","\n","# --- Re-execute Dataset Creation (from 3907940b) ---\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","BATCH_SIZE = 32\n","\n","def preprocess_image(image_path, label):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n","    image = (image / 127.5) - 1\n","    return image, label\n","\n","def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(image_paths))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","train_dataset = create_dataset(train_image_paths, train_labels, shuffle=True)\n","val_dataset = create_dataset(val_image_paths, val_labels, shuffle=False)\n","test_dataset = create_dataset(test_image_paths, test_labels, shuffle=False)\n","\n","print(\"Datasets (train_dataset, val_dataset, test_dataset) have been successfully re-created.\")\n","\n","# --- Model Training ---\n","EPOCHS = 10 # Define the number of training epochs\n","\n","# Train the model\n","print(f\"\\nStarting model training for {EPOCHS} epochs...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"\\nModel training completed for {EPOCHS} epochs.\")"],"execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/annotations/annotations/list.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1987928752.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbreed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/annotations/list.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"0ba1727f"},"source":["**Reasoning**:\n","The previous code block failed because the necessary dataset files, specifically `list.txt`, were not found. This indicates that the dataset was not correctly unzipped or the unzipped files were not available in the expected location. I will re-run the `unzip` command to ensure the dataset is extracted, and then proceed with re-creating the datasets (train_dataset, val_dataset, test_dataset) and finally, start the model training.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343},"id":"66582510","executionInfo":{"status":"error","timestamp":1770091161524,"user_tz":-420,"elapsed":150,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"f4adb87d-7300-4bbd-e3cb-2586617c8df3"},"source":["!unzip -q '/content/drive/MyDrive/datasetoxfordclass.zip' -d '/content/'\n","\n","import os\n","import re\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","# --- Re-execute Data Parsing and Splitting (from f0e9cee2) ---\n","annotations_dir = '/content/annotations/annotations/'\n","list_file_path = os.path.join(annotations_dir, 'list.txt')\n","\n","def extract_breed_name(image_name_prefix):\n","    return '_'.join(image_name_prefix.split('_')[:-1])\n","\n","breed_names = []\n","with open(list_file_path, 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line.startswith('#'):\n","            continue\n","        parts = line.split(' ')\n","        if parts:\n","            image_name_prefix = parts[0]\n","            breed_name = extract_breed_name(image_name_prefix)\n","            if breed_name:\n","                breed_names.append(breed_name)\n","\n","unique_breed_names = sorted(list(set(breed_names)))\n","num_classes = len(unique_breed_names)\n","\n","class_to_label = {name: i for i, name in enumerate(unique_breed_names)}\n","label_to_class = {i: name for i, name in enumerate(unique_breed_names)}\n","\n","image_base_path = '/content/images/images/'\n","\n","def parse_annotation_file(file_path, class_to_label):\n","    image_paths = []\n","    labels = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('#'):\n","                continue\n","            parts = line.split(' ')\n","            if len(parts) >= 1:\n","                image_name_prefix = parts[0]\n","                breed_name = extract_breed_name(image_name_prefix)\n","\n","                if breed_name in class_to_label:\n","                    class_idx = class_to_label[breed_name]\n","                    full_image_path = os.path.join(image_base_path, image_name_prefix + '.jpg')\n","\n","                    if os.path.exists(full_image_path):\n","                        image_paths.append(full_image_path)\n","                        labels.append(class_idx)\n","    return image_paths, labels\n","\n","trainval_file_path = os.path.join(annotations_dir, 'trainval.txt')\n","test_file_path = os.path.join(annotations_dir, 'test.txt')\n","\n","trainval_image_paths, trainval_labels = parse_annotation_file(trainval_file_path, class_to_label)\n","test_image_paths, test_labels = parse_annotation_file(test_file_path, class_to_label)\n","\n","train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n","    trainval_image_paths, trainval_labels, test_size=0.2, random_state=42, stratify=trainval_labels\n",")\n","\n","# --- Re-execute Dataset Creation (from 3907940b) ---\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","BATCH_SIZE = 32\n","\n","def preprocess_image(image_path, label):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n","    image = (image / 127.5) - 1\n","    return image, label\n","\n","def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(image_paths))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","train_dataset = create_dataset(train_image_paths, train_labels, shuffle=True)\n","val_dataset = create_dataset(val_image_paths, val_labels, shuffle=False)\n","test_dataset = create_dataset(test_image_paths, test_labels, shuffle=False)\n","\n","print(\"Datasets (train_dataset, val_dataset, test_dataset) have been successfully re-created.\")\n","\n","# --- Model Training ---\n","EPOCHS = 10 # Define the number of training epochs\n","\n","# Train the model\n","print(f\"\\nStarting model training for {EPOCHS} epochs...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"\\nModel training completed for {EPOCHS} epochs.\")"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["unzip:  cannot find or open /content/drive/MyDrive/datasetoxfordclass.zip, /content/drive/MyDrive/datasetoxfordclass.zip.zip or /content/drive/MyDrive/datasetoxfordclass.zip.ZIP.\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/annotations/annotations/list.txt'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3185484035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mbreed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/annotations/annotations/list.txt'"]}]},{"cell_type":"markdown","metadata":{"id":"ad82997e"},"source":["**Reasoning**:\n","The previous code block failed because the `unzip` command could not find the dataset file, leading to subsequent `FileNotFoundError` for `list.txt`. I will re-add `drive.mount` with `force_remount=True` for robustness, then re-attempt the `unzip` command. Following this, I will re-declare necessary variables and re-execute the data parsing, dataset creation, and model training steps, assuming the `model` object was successfully compiled in an earlier step and is still in scope.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"37eb0e73","executionInfo":{"status":"ok","timestamp":1770091404257,"user_tz":-420,"elapsed":207872,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"5a216ff2-0f85-45bc-f4cd-d35f589b7736"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","!unzip -q '/content/drive/MyDrive/datasetoxfordclass.zip' -d '/content/'\n","\n","import os\n","import re\n","import tensorflow as tf\n","from sklearn.model_selection import train_test_split\n","\n","# --- Re-define crucial variables from previous successful steps ---\n","# These values were determined in cells 3907940b and f0e9cee2 respectively.\n","IMG_HEIGHT = 224\n","IMG_WIDTH = 224\n","num_classes = 37 # This was determined after correctly parsing breed names\n","BATCH_SIZE = 32 # Ensure BATCH_SIZE is also defined\n","\n","# --- Re-execute Data Parsing and Splitting (from f0e9cee2) ---\n","annotations_dir = '/content/annotations/annotations/'\n","list_file_path = os.path.join(annotations_dir, 'list.txt')\n","\n","def extract_breed_name(image_name_prefix):\n","    return '_'.join(image_name_prefix.split('_')[:-1])\n","\n","breed_names = []\n","with open(list_file_path, 'r') as f:\n","    for line in f:\n","        line = line.strip()\n","        if line.startswith('#'):\n","            continue\n","        parts = line.split(' ')\n","        if parts:\n","            image_name_prefix = parts[0]\n","            breed_name = extract_breed_name(image_name_prefix)\n","            if breed_name:\n","                breed_names.append(breed_name)\n","\n","unique_breed_names = sorted(list(set(breed_names)))\n","num_classes = len(unique_breed_names) # Ensure num_classes is consistent\n","\n","class_to_label = {name: i for i, name in enumerate(unique_breed_names)}\n","label_to_class = {i: name for i, name in enumerate(unique_breed_names)}\n","\n","image_base_path = '/content/images/images/'\n","\n","def parse_annotation_file(file_path, class_to_label):\n","    image_paths = []\n","    labels = []\n","    with open(file_path, 'r') as f:\n","        for line in f:\n","            line = line.strip()\n","            if line.startswith('#'):\n","                continue\n","            parts = line.split(' ')\n","            if len(parts) >= 1:\n","                image_name_prefix = parts[0]\n","                breed_name = extract_breed_name(image_name_prefix)\n","\n","                if breed_name in class_to_label:\n","                    class_idx = class_to_label[breed_name]\n","                    full_image_path = os.path.join(image_base_path, image_name_prefix + '.jpg')\n","\n","                    if os.path.exists(full_image_path):\n","                        image_paths.append(full_image_path)\n","                        labels.append(class_idx)\n","    return image_paths, labels\n","\n","trainval_file_path = os.path.join(annotations_dir, 'trainval.txt')\n","test_file_path = os.path.join(annotations_dir, 'test.txt')\n","\n","trainval_image_paths, trainval_labels = parse_annotation_file(trainval_file_path, class_to_label)\n","test_image_paths, test_labels = parse_annotation_file(test_file_path, class_to_label)\n","\n","train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n","    trainval_image_paths, trainval_labels, test_size=0.2, random_state=42, stratify=trainval_labels\n",")\n","\n","# --- Re-execute Dataset Creation (from 3907940b) ---\n","\n","def preprocess_image(image_path, label):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=3)\n","    image = tf.image.resize(image, [IMG_HEIGHT, IMG_WIDTH])\n","    image = (image / 127.5) - 1\n","    return image, label\n","\n","def create_dataset(image_paths, labels, batch_size=BATCH_SIZE, shuffle=True, augment=False):\n","    dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n","    dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n","    if shuffle:\n","        dataset = dataset.shuffle(buffer_size=len(image_paths))\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n","    return dataset\n","\n","train_dataset = create_dataset(train_image_paths, train_labels, shuffle=True)\n","val_dataset = create_dataset(val_image_paths, val_labels, shuffle=False)\n","test_dataset = create_dataset(test_image_paths, test_labels, shuffle=False)\n","\n","print(\"Datasets (train_dataset, val_dataset, test_dataset) have been successfully re-created.\")\n","\n","# --- Model Training ---\n","EPOCHS = 10 # Define the number of training epochs\n","\n","# Train the model\n","print(f\"\\nStarting model training for {EPOCHS} epochs...\")\n","history = model.fit(\n","    train_dataset,\n","    epochs=EPOCHS,\n","    validation_data=val_dataset\n",")\n","\n","print(f\"\\nModel training completed for {EPOCHS} epochs.\")"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Datasets (train_dataset, val_dataset, test_dataset) have been successfully re-created.\n","\n","Starting model training for 10 epochs...\n","Epoch 1/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 107ms/step - accuracy: 0.5436 - loss: 1.8243 - val_accuracy: 0.8791 - val_loss: 0.3854\n","Epoch 2/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 62ms/step - accuracy: 0.9592 - loss: 0.1711 - val_accuracy: 0.8913 - val_loss: 0.3787\n","Epoch 3/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 49ms/step - accuracy: 0.9784 - loss: 0.1016 - val_accuracy: 0.8832 - val_loss: 0.3617\n","Epoch 4/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9911 - loss: 0.0584 - val_accuracy: 0.9049 - val_loss: 0.3031\n","Epoch 5/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 64ms/step - accuracy: 0.9967 - loss: 0.0304 - val_accuracy: 0.9008 - val_loss: 0.3175\n","Epoch 6/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 55ms/step - accuracy: 0.9987 - loss: 0.0205 - val_accuracy: 0.9008 - val_loss: 0.3257\n","Epoch 7/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 57ms/step - accuracy: 0.9988 - loss: 0.0179 - val_accuracy: 0.8981 - val_loss: 0.3145\n","Epoch 8/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 0.9062 - val_loss: 0.3143\n","Epoch 9/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.9008 - val_loss: 0.3192\n","Epoch 10/10\n","\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.0072 - val_accuracy: 0.8995 - val_loss: 0.3222\n","\n","Model training completed for 10 epochs.\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7ef7aee","executionInfo":{"status":"ok","timestamp":1770091685724,"user_tz":-420,"elapsed":501,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"f3711033-89f6-4796-a6aa-bd5a14073d31"},"source":["import os\n","\n","# Define a path to save the model weights, correcting the filename to end with .weights.h5\n","weights_save_path = './fine_tuned_mobilenetv2.weights.h5'\n","\n","# Save only the model weights\n","model.save_weights(weights_save_path)\n","\n","print(f\"Model weights saved successfully to {weights_save_path}\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Model weights saved successfully to ./fine_tuned_mobilenetv2.weights.h5\n"]}]},{"cell_type":"markdown","metadata":{"id":"9054c4e8"},"source":["## Đánh giá mô hình\n","\n","### Subtask:\n","Đánh giá hiệu suất của mô hình đã tinh chỉnh trên tập xác thực hoặc tập kiểm tra.\n"]},{"cell_type":"markdown","metadata":{"id":"6246b016"},"source":["**Reasoning**:\n","As per the instructions, I need to evaluate the trained model on the `test_dataset` using the `.evaluate()` method to get the test loss and test accuracy. The `model` and `test_dataset` are already prepared.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a2457eec","executionInfo":{"status":"ok","timestamp":1770091463008,"user_tz":-420,"elapsed":23327,"user":{"displayName":"Hiệp Trương Viết","userId":"13759110486598229769"}},"outputId":"edd4d687-f087-4dd6-ae5a-15001d681612"},"source":["print(\"\\nEvaluating model on the test dataset...\")\n","loss, accuracy = model.evaluate(test_dataset)\n","\n","print(f\"Test Loss: {loss:.4f}\")\n","print(f\"Test Accuracy: {accuracy:.4f}\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Evaluating model on the test dataset...\n","\u001b[1m115/115\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 202ms/step - accuracy: 0.8741 - loss: 0.4593\n","Test Loss: 0.3663\n","Test Accuracy: 0.8970\n"]}]},{"cell_type":"markdown","metadata":{"id":"77046dcf"},"source":["## Final Task\n","\n","### Subtask:\n","Tóm tắt quá trình tinh chỉnh và kết quả đạt được từ mô hình MobileNetV2.\n"]},{"cell_type":"markdown","metadata":{"id":"e68ab1c2"},"source":["## Summary:\n","\n","### Data Analysis Key Findings\n","\n","*   **GPU Availability:** The environment was successfully configured with a GPU (Tesla T4 with 15360MiB memory and CUDA 12.4), enabling accelerated model training.\n","*   **Data Preparation:** The necessary datasets (`train_dataset`, `val_dataset`, `test_dataset`) were successfully created, following the re-mounting of Google Drive and extraction of the `datasetoxfordclass.zip` file, which contained annotations and images for 37 distinct pet breeds.\n","*   **Model Architecture:** A MobileNetV2 base model (pre-trained on ImageNet) was used, with its layers frozen, and a new classification head (GlobalAveragePooling2D, Dense(128, relu), Dense(37, softmax)) was added for breed classification.\n","*   **Training Performance:** The model was trained for 10 epochs using an Adam optimizer and SparseCategoricalCrossentropy loss.\n","    *   **Initial Performance (Epoch 1):** The model started with a training loss of 1.8243 and accuracy of 0.5436. Validation performance was strong, with a loss of 0.3854 and accuracy of 0.8791.\n","    *   **Final Training Performance (Epoch 10):** Training loss significantly decreased to 0.0072, and training accuracy reached 1.0000. Validation loss improved to 0.3222, and validation accuracy reached 0.8995.\n","*   **Test Set Performance:** The fine-tuned model achieved a Test Loss of 0.3663 and a Test Accuracy of 0.8970 on unseen data.\n","\n","### Insights or Next Steps\n","\n","*   The transfer learning approach with MobileNetV2 and a custom classification head proved highly effective, achieving nearly 90% accuracy on both validation and test datasets within just 10 epochs.\n","*   Further improvements could be explored by unfreezing some of the MobileNetV2 base layers and continuing fine-tuning with a lower learning rate, or by implementing data augmentation techniques during training to potentially enhance generalization and prevent overfitting to the training set.\n"]}]}